--- 
title: Home 
layout: default 
---


<h1>Introduction to RLPark</h1>

RLPark is a Java framework to run reinforcement learning algorithms on robots and benchmarks. 
RLPark uses <a href="http://thomasdegris.github.com/zephyr">Zephyr</a>
for visualization and real-time display. It is also compatible with the Eclipse RCP framework.

<br/>
<br/>
<b>Demos</b> of RLPark are available in Zephyr. To install them, download 
<a href="http://thomasdegris.github.com/zephyr/download.html">Zephyr standalone application</a>, then 
<a href="http://thomasdegris.github.com/rlpark/download.html"> install all the RLPark features</a> in Zephyr.
After the installation, a new <span class='menu'>Demos</span> menu will appear from which you can start the demos.  

<div align="center">
	<iframe width="420" height="315"
		src="http://www.youtube.com/v/SU3C_37DzYg" frameborder="0"
		allowfullscreen></iframe>
</div>

<h4>RLPark features and algorithms:</h4>
<ul>
	<li>On-policy control: Sarsa(λ), Expected Sarsa(λ), Actor-Critic with 
	normal distribution (continuous actions) and Boltzmann distribution (discrete action),
	average reward actor-critic</li>
	<li>Off-policy control: Off-PAC, Q-Learning, Q(λ), Greedy-GQ, Softmax-GQ</li>
	<li>On-policy prediction: TD, TD(λ)</li>
	<li>Off-policy prediction: GTD(λ), GQ(λ), TDC</li>
	<li>Acting: softmax, greedy, ε-greedy</li>
	<li>Online supervised learning: Adaline, IDBD</li>
	<li>Representations and function approximation: tile coding (with no hashing, hashing with UNH CMAC or mumur2), 
	    Linear Threshold Unit, Radial Basis Function, observation history, feature normalization</li>
	<li>Robots: the <a href="https://sites.google.com/a/rl-community.org/critterbot/hardware/the-critterbot-robot">Critterbot</a>,
		<a href="https://sites.google.com/a/rl-community.org/critterbot/hardware/the-irobot-create">iRobot Create</a></li>
	<li>Compatible with <a href="http://thomasdegris.github.com/zephyr">Zephyr</a></li>
	<li>Problems: mountain car, swing-up pendulum, random walk, continuous world</li>
	<li>Eligibility traces: accumulating traces, accumulating traces with a max, replacing traces</li>
	<li>The Horde architecture</li>
	<li>A framework for running parallel code on clusters</li>
	<li>A framework for on-policy control experiments and off-policy control experiments</li>
</ul>
<ul>
</ul>
To learn more about Reinforcement Learning, I recommend the following two books (both available online):
<ul>
	<li><a href="http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html">Reinforcement Learning: An Introduction</a> (Sutton & Barto, 1998)</li>
	<li><a href="http://www.ualberta.ca/~szepesva/RLBook.html">Algorithms for Reinforcement Learning</a> (Szepesv&aacute;ri, 2010)
</ul>

<h4>Contributors</h4>

Author: Thomas Degris</br>
Contributors: Adam White, Joseph Modayil, Patrick M. Pilarski, Christian Denk
