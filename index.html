--- 
title: Home 
layout: default 
---


<h1>Introduction to RLPark</h1>

RLPark is a reinforcement learning library in Java to experiment with 
online learning algorithms on robots and benchmarks. 
RLPark uses <a href="http://zephyrplugins.github.com">Zephyr</a>
for visualization and real-time display. 

<br/>
<br/>
<b>Demos</b> of RLPark <i>without visualization</i> can be run directly just 
using <a href="http://www.ualberta.ca/~degris/rlpark.jar">rlpark.jar</a>.
Demos of RLPark <i>with visualization</i> are available in Zephyr. To install them, download 
<a href="http://zephyrplugins.github.com/download.html">Zephyr standalone application</a>, then 
<a href="http://rlpark.github.com/download.html"> install all the RLPark features</a> in Zephyr.
After the installation, a new <span class='menu'>Demos</span> menu will appear from which you can start the demos.
See the <a href="documentation.html">documentation</a> for more information.
<br/>

<div align="center">
	<iframe width="420" height="315"
		src="http://www.youtube.com/v/SU3C_37DzYg" frameborder="0"
		allowfullscreen></iframe>
</div>

<h4>RLPark features and algorithms:</h4>
<ul>
	<li>On-policy control: Sarsa(λ), Expected Sarsa(λ), Actor-Critic with 
	normal distribution (continuous actions) and Boltzmann distribution (discrete action),
	average reward actor-critic</li>
	<li>Off-policy control: Off-PAC, Q-Learning, Q(λ), Greedy-GQ, Softmax-GQ</li>
	<li>On-policy prediction: TD, TD(λ)</li>
	<li>Off-policy prediction: GTD(λ), GQ(λ), TDC</li>
	<li>Acting: softmax, greedy, ε-greedy</li>
	<li>Online supervised learning: Adaline, IDBD</li>
	<li>Representations and function approximation: tile coding (with no hashing, hashing with UNH CMAC or mumur2), 
	    Linear Threshold Unit, Radial Basis Function, observation history, feature normalization</li>
	<li>Robots: the <a href="https://sites.google.com/a/rl-community.org/critterbot/hardware/the-critterbot-robot">Critterbot</a>,
		<a href="https://sites.google.com/a/rl-community.org/critterbot/hardware/the-irobot-create">iRobot Create</a></li>
	<li>Compatible with <a href="http://zephyrplugins.github.com">Zephyr</a></li>
	<li>Problems: mountain car, swing-up pendulum, random walk, continuous world</li>
	<li>Eligibility traces: accumulating traces, accumulating traces with a max, replacing traces</li>
	<li>The Horde architecture (see <a href='http://code.google.com/p/opencl-horde/'>OpenCL-Horde</a> for an implementation on GPU by Clement Gehring)</li>
	<li>A framework for running parallel code on clusters</li>
	<li>A framework for on-policy control experiments and off-policy control experiments</li>
</ul>
<h4>RLPark related projects:</h4>
<ul>
	<li><a href='http://code.google.com/p/opencl-horde/'>OpenCL-Horde</a>: implementation of Horde on GPU by Clement Gehring</li>
	<li><a href='http://web.cs.miami.edu/home/saminda/rllib.html'>RLLib</a>: port of RLPark to C++ by Saminda Abeyruwan</li>
</ul>
To learn more about Reinforcement Learning, the following two books are available online:
<ul>
	<li><a href="http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html">Reinforcement Learning: An Introduction</a> (Sutton & Barto, 1998)</li>
	<li><a href="http://www.ualberta.ca/~szepesva/RLBook.html">Algorithms for Reinforcement Learning</a> (Szepesv&aacute;ri, 2010)
</ul>

<h4>Contributors</h4>

Main author: Thomas Degris<br/>
Contributors: Jérome Béchu, Adam White, Joseph Modayil, Patrick M. Pilarski, Christian Denk
